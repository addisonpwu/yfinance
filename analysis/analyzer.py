import yfinance as yf
import pandas as pd
import pkgutil
import importlib
import inspect
import os
import json
import subprocess
import re
import time
from datetime import datetime, timedelta, date
from strategies.base_strategy import BaseStrategy
from data_loader import us_loader, hk_loader
from ai_analyzer import analyze_stock_with_ai
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import numpy as np

def parse_kronos_prediction(prediction_text: str) -> tuple[float, float]:
    """
    è§£æ Kronos é¢„æµ‹è¾“å‡ºï¼Œæå–ä¸Šå‡å’Œä¸‹è·Œæœºç‡

    Args:
        prediction_text: Kronos é¢„æµ‹è„šæœ¬çš„è¾“å‡ºæ–‡æœ¬

    Returns:
        (ä¸Šå‡æœºç‡, ä¸‹è·Œæœºç‡) çš„å…ƒç»„ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å› (0, 0)
    """
    try:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æœºç‡
        rise_match = re.search(r'åƒ¹æ ¼ä¸Šå‡æ©Ÿç‡:\s*([\d.]+)%', prediction_text)
        fall_match = re.search(r'åƒ¹æ ¼ä¸‹è·Œæ©Ÿç‡:\s*([\d.]+)%', prediction_text)

        if rise_match and fall_match:
            rise_prob = float(rise_match.group(1))
            fall_prob = float(fall_match.group(1))
            return rise_prob, fall_prob
        else:
            return 0.0, 0.0
    except Exception as e:
        print(f"è§£æ Kronos é¢„æµ‹æœºç‡æ—¶å‡ºé”™: {e}")
        return 0.0, 0.0

def get_strategies():
    """
    å‹•æ…‹å¾ strategies æ¨¡çµ„åŠ è¼‰æ‰€æœ‰ç­–ç•¥é¡åˆ¥çš„å¯¦ä¾‹ã€‚
    """
    strategies = []
    import strategies as strategies_module
    strategy_path = strategies_module.__path__

    for _, name, _ in pkgutil.iter_modules(strategy_path):
        if name != 'base_strategy':
            module = importlib.import_module(f"strategies.{name}")
            for item_name, item in inspect.getmembers(module, inspect.isclass):
                if issubclass(item, BaseStrategy) and item is not BaseStrategy:
                    strategies.append(item())
    return strategies

def _read_csv_with_auto_index(csv_file: str) -> pd.DataFrame:
    """
    è¯»å– CSV æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•åˆ—åï¼ˆDate æˆ– Datetimeï¼‰

    Args:
        csv_file: CSV æ–‡ä»¶è·¯å¾„

    Returns:
        DataFrame
    """
    # å…ˆè¯»å–ç¬¬ä¸€è¡Œæ¥æ£€æµ‹åˆ—å
    with open(csv_file, 'r') as f:
        first_line = f.readline()
    
    # æ£€æµ‹ç´¢å¼•åˆ—å
    if 'Datetime,' in first_line:
        index_col = 'Datetime'
    else:
        index_col = 'Date'
    
    # ä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•åˆ—åè¯»å–
    return pd.read_csv(csv_file, index_col=index_col, parse_dates=True)

def load_config():
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    """
    config_path = os.path.join(os.path.dirname(__file__), '..', 'config.json')
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        print(f"é…ç½®æ–‡ä»¶ {config_path} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        return {
            "api": {
                "base_delay": 0.5,
                "max_delay": 2.0,
                "min_delay": 0.1,
                "retry_attempts": 3,
                "max_workers": 4
            },
            "data": {
                "max_cache_days": 7,
                "float_dtype": "float32"
            },
            "analysis": {
                "enable_realtime_output": true,
                "enable_data_preprocessing": true,
                "min_volume_threshold": 100000
            }
        }

def optimize_dataframe_memory(df: pd.DataFrame) -> pd.DataFrame:
    """
    ä¼˜åŒ– DataFrame çš„å†…å­˜ä½¿ç”¨ï¼Œé€šè¿‡ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç±»å‹

    Args:
        df: åŸå§‹ DataFrame

    Returns:
        ä¼˜åŒ–åçš„ DataFrame
    """
    df_optimized = df.copy()
    
    for col in df_optimized.columns:
        col_type = df_optimized[col].dtype
        
        if col_type != 'object':
            c_min = df_optimized[col].min()
            c_max = df_optimized[col].max()
            
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df_optimized[col] = df_optimized[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df_optimized[col] = df_optimized[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df_optimized[col] = df_optimized[col].astype(np.int32)
                else:
                    df_optimized[col] = df_optimized[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df_optimized[col] = df_optimized[col].astype(np.float32)
                else:
                    df_optimized[col] = df_optimized[col].astype(np.float64)
    
    return df_optimized

def serialize_for_json(obj):
    """
    å°†å¯¹è±¡è½¬æ¢ä¸ºå¯ JSON åºåˆ—åŒ–çš„æ ¼å¼ï¼ˆé€’å½’å¤„ç†æ‰€æœ‰å±‚çº§ï¼‰

    Args:
        obj: è¦åºåˆ—åŒ–çš„å¯¹è±¡

    Returns:
        å¯ JSON åºåˆ—åŒ–çš„å¯¹è±¡
    """
    import pandas as pd
    import numpy as np
    from datetime import datetime, date

    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸º NaNï¼ˆæ ‡é‡ï¼‰
    try:
        if isinstance(obj, (float, int)) and np.isnan(obj):
            return None
    except (TypeError, ValueError):
        pass

    # å¤„ç†æ—¥æœŸæ—¶é—´å¯¹è±¡
    if isinstance(obj, (pd.Timestamp, datetime, date)):
        return obj.isoformat()

    # å¤„ç† Series æˆ– DataFrame - å…ˆè½¬æ¢ä¸ºå­—å…¸ï¼Œç„¶åé€’å½’å¤„ç†
    elif isinstance(obj, (pd.Series, pd.DataFrame)):
        result_dict = obj.to_dict()
        return serialize_for_json(result_dict)  # å…³é”®ï¼šé€’å½’å¤„ç†è½¬æ¢åçš„å­—å…¸

    # å¤„ç†å­—å…¸ - é€’å½’å¤„ç†é”®å’Œå€¼
    elif isinstance(obj, dict):
        result = {}
        for k, v in obj.items():
            # å¤„ç†é”®
            if isinstance(k, (pd.Timestamp, datetime, date)):
                k = k.isoformat()
            # é€’å½’å¤„ç†å€¼
            result[k] = serialize_for_json(v)
        return result

    # å¤„ç†åˆ—è¡¨/å…ƒç»„ - é€’å½’å¤„ç†æ¯ä¸ªå…ƒç´ 
    elif isinstance(obj, (list, tuple)):
        return [serialize_for_json(item) for item in obj]

    # å¤„ç† numpy ç±»å‹
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()

    # å…¶ä»–ç±»å‹ç›´æ¥è¿”å›
    else:
        return obj

def get_enhanced_financial_data(ticker: yf.Ticker) -> dict:
    """
    è·å–å¢å¼ºçš„è´¢åŠ¡æ•°æ®ï¼ŒåŒ…æ‹¬è´¢åŠ¡æŠ¥è¡¨å’Œå…³é”®æŒ‡æ ‡

    Args:
        ticker: yfinance Ticker å¯¹è±¡

    Returns:
        åŒ…å«å¢å¼ºè´¢åŠ¡æ•°æ®çš„å­—å…¸
    """
    enhanced_data = {}

    try:
        # åªè·å–æœ€å…³é”®çš„è´¢åŠ¡æŠ¥è¡¨æ•°æ®ï¼Œå‡å°‘ API è°ƒç”¨
        financials = ticker.financials
        if financials is not None and isinstance(financials, pd.DataFrame) and not financials.empty:
            enhanced_data['financials'] = financials.to_dict()

        # è·å–èµ„äº§è´Ÿå€ºè¡¨
        balance_sheet = ticker.balance_sheet
        if balance_sheet is not None and isinstance(balance_sheet, pd.DataFrame) and not balance_sheet.empty:
            enhanced_data['balance_sheet'] = balance_sheet.to_dict()

        # è·å–ç°é‡‘æµé‡è¡¨
        cashflow = ticker.cashflow
        if cashflow is not None and isinstance(cashflow, pd.DataFrame) and not cashflow.empty:
            enhanced_data['cashflow'] = cashflow.to_dict()

    except Exception as e:
        print(f" - [å¢å¼ºæ•°æ®] è·å–å¤±è´¥: {e}", end='')

    return enhanced_data

def get_data_with_cache(symbol: str, market: str, fast_mode: bool = False, interval: str = '1d') -> (pd.DataFrame, dict, dict):
    """
    ç²å–è‚¡ç¥¨æ•¸æ“šï¼Œæ ¹æ“šæ¨¡å¼é¸æ“‡å¿«é€ŸåŠ è¼‰æˆ–åŒæ­¥æ›´æ–°ã€‚

    Args:
        symbol: è‚¡ç¥¨ä»£ç¢¼
        market: å¸‚å ´ä»£ç¢¼ ('US' æˆ– 'HK')
        fast_mode: æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæ¨¡å¼
        interval: æ•¸æ“šæ™‚æ®µé¡å‹ ('1d' æ—¥ç·š, '1h' å°æ™‚ç·š, '1m' åˆ†é˜ç·š)
    """
    cache_dir = os.path.join('data_cache', market.upper())
    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
    os.makedirs(cache_dir, exist_ok=True)
    safe_symbol = symbol.replace(":", "_")
    csv_file = os.path.join(cache_dir, f"{safe_symbol}_{interval}.csv")  # æ·»åŠ  interval åˆ°æ–‡ä»¶å
    json_file = os.path.join(cache_dir, f"{safe_symbol}.json")

    ticker = yf.Ticker(symbol)

    if fast_mode:
        try:
            # print(f" - [å¿«é€Ÿæ¨¡å¼] å¾ç·©å­˜åŠ è¼‰", end='')
            # è‡ªåŠ¨æ£€æµ‹ç´¢å¼•åˆ—åï¼ˆDate æˆ– Datetimeï¼‰
            hist = _read_csv_with_auto_index(csv_file)
            with open(json_file, 'r', encoding='utf-8') as f:
                info = json.load(f)

            # ç¡®ä¿ info æ˜¯å­—å…¸
            if not isinstance(info, dict):
                info = {}

            # éªŒè¯å…³é”®å­—æ®µ
            required_fields = [
                'marketCap', 'trailingPE', 'forwardPE', 'pegRatio', 'priceToBook',
                'profitMargins', 'returnOnEquity', 'revenueGrowth', 'earningsGrowth',
                'dividendYield', 'beta', '52WeekChange', 'targetMeanPrice',
                'volume', 'floatShares', 'shortRatio'
            ]
            for field in required_fields:
                if field not in info:
                    info[field] = None

            # ç§»é™¤ news è°ƒç”¨ä»¥å‡å°‘ API è¯·æ±‚
            news = []
            # ä¼˜åŒ–å†…å­˜ä½¿ç”¨ - è½¬æ¢æ•°æ®ç±»å‹
            hist = optimize_dataframe_memory(hist)
            return hist, info, news
        except FileNotFoundError:
            # print(f" - [å¿«é€Ÿæ¨¡å¼] ç·©å­˜æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œåˆ‡æ›åˆ°æ­£å¸¸æ¨¡å¼ä¸‹è¼‰", end='')
            return get_data_with_cache(symbol, market, fast_mode=False, interval=interval)
        except (json.JSONDecodeError, ValueError) as e:
            print(f" - [å¿«é€Ÿæ¨¡å¼] JSON è§£æå¤±è´¥: {e}ï¼Œé‡æ–°ä¸‹è¼‰", end='')
            # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
            try:
                os.remove(json_file)
            except:
                pass
            return get_data_with_cache(symbol, market, fast_mode=False, interval=interval)

    # --- æ­£å¸¸åŒæ­¥æ¨¡å¼ ---
    today = datetime.now().date()
    hist, info, news = pd.DataFrame(), {}, []

    # é¦–å…ˆè·å–å†å²ä»·æ ¼æ•°æ®ï¼ˆè¿™ä¸ªé€šå¸¸æ¯” info æ›´å®¹æ˜“è·å–ï¼‰
    if os.path.exists(csv_file):
        # è‡ªåŠ¨æ£€æµ‹ç´¢å¼•åˆ—åï¼ˆDate æˆ– Datetimeï¼‰
        hist = _read_csv_with_auto_index(csv_file)
        last_cached_date = hist.index.max().date()

        if last_cached_date >= today:
            print(f" - å¾ç·©å­˜åŠ è¼‰ {len(hist)} æ¢æ•¸æ“š", end='')
        else:
            start_date = last_cached_date + timedelta(days=1)
            print(f" - ç·©å­˜æ•¸æ“šéèˆŠï¼Œæ­£åœ¨å¾ {start_date.strftime('%Y-%m-%d')} ä¸‹è¼‰å¢é‡æ•¸æ“š...", end='')
            new_hist = ticker.history(start=start_date.strftime('%Y-%m-%d'), interval=interval, auto_adjust=True)
            if not new_hist.empty:
                hist = pd.concat([hist, new_hist])
                print(f"ä¸‹è¼‰äº† {len(new_hist)} æ¢æ–°æ•¸æ“š", end='')
            else:
                print("æ²’æœ‰æ–°çš„æ•¸æ“šå¯ä¸‹è¼‰", end='')
    else:
        print(" - ç·©å­˜ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä¸‹è¼‰å…¨éƒ¨æ­·å²æ•¸æ“š...", end='')
        # æ ¹æ“š interval è¨­ç½®ä¸åŒçš„ period
        if interval == '1m':
            period = '7d'  # åˆ†é˜ç·šåªä¸‹è¼‰æœ€è¿‘7å¤©
        elif interval == '1h':
            period = '730d'  # å°æ™‚ç·šä¸‹è¼‰æœ€è¿‘2å¹´
        else:
            period = 'max'  # æ—¥ç·šä¸‹è¼‰å…¨éƒ¨æ­·å²
        hist = ticker.history(period=period, interval=interval, auto_adjust=True)
        print(f"ä¸‹è¼‰äº† {len(hist)} æ¢æ•¸æ“š", end='')

    # ä¼˜åŒ–å†…å­˜ä½¿ç”¨ - è½¬æ¢æ•°æ®ç±»å‹
    if not hist.empty:
        hist = optimize_dataframe_memory(hist)
        float_shares = None  # æš‚æ—¶è®¾ç½®ä¸º None
        hist['FloatShares'] = float_shares
        hist.to_csv(csv_file)

    # å°è¯•è·å– info æ•°æ®ï¼ˆå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç©ºå­—å…¸ï¼‰
    try:
        info = ticker.info
        # ç¡®ä¿ info ä¸ä¸ºç©º - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼
        if info is None:
            print(f" - info æ•°æ®ä¸ºç©º", end='')
            info = {}
        elif not isinstance(info, dict):
            # å¦‚æœ info ä¸æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºå­—å…¸
            print(f" - info æ ¼å¼å¼‚å¸¸ï¼Œè½¬æ¢ä¸ºå­—å…¸", end='')
            info = {}
        elif isinstance(info, dict) and len(info) == 0:
            print(f" - info å­—å…¸ä¸ºç©º", end='')
            # ä¿æŒä¸ºç©ºå­—å…¸ï¼Œç»§ç»­å°è¯•è·å–å¢å¼ºæ•°æ®

        # éªŒè¯å…³é”®å­—æ®µæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¾ç½®ä¸º None
        required_fields = [
            'marketCap', 'trailingPE', 'forwardPE', 'pegRatio', 'priceToBook',
            'profitMargins', 'returnOnEquity', 'revenueGrowth', 'earningsGrowth',
            'dividendYield', 'beta', '52WeekChange', 'targetMeanPrice',
            'volume', 'floatShares', 'shortRatio'
        ]
        for field in required_fields:
            if field not in info:
                info[field] = None

        # è·å–å¢å¼ºçš„è´¢åŠ¡æ•°æ®
        enhanced_data = get_enhanced_financial_data(ticker)
        if enhanced_data:
            info['enhanced_financial_data'] = enhanced_data

        # ä¿å­˜ info åˆ°ç¼“å­˜ - åªåœ¨æœ‰æœ‰æ•ˆæ•°æ®æ—¶ä¿å­˜
        if isinstance(info, dict) and len(info) > 0:
            try:
                # ä½¿ç”¨é€’å½’çš„ serialize_for_json å¤„ç†æ‰€æœ‰åµŒå¥—å±‚çº§
                processed_info = serialize_for_json(info)

                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(processed_info, f, ensure_ascii=False, indent=4)
            except Exception as save_error:
                print(f" - ä¿å­˜ info å¤±è´¥: {save_error}", end='')
                # ä¿å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹

    except Exception as e:
        print(f" - ç„¡æ³•ç²å– info: {e}ï¼Œå°‡ä½¿ç”¨ç©ºæ•¸æ“š", end='')
        info = {}

    news = []

    if os.path.exists(csv_file):
        # è‡ªåŠ¨æ£€æµ‹ç´¢å¼•åˆ—åï¼ˆDate æˆ– Datetimeï¼‰
        hist = _read_csv_with_auto_index(csv_file)
        last_cached_date = hist.index.max().date()

        if last_cached_date >= today:
            print(f" - å¾ç·©å­˜åŠ è¼‰ {len(hist)} æ¢æ•¸æ“š", end='')
        else:
            start_date = last_cached_date + timedelta(days=1)
            print(f" - ç·©å­˜æ•¸æ“šéèˆŠï¼Œæ­£åœ¨å¾ {start_date.strftime('%Y-%m-%d')} ä¸‹è¼‰å¢é‡æ•¸æ“š...", end='')
            new_hist = ticker.history(start=start_date.strftime('%Y-%m-%d'), interval=interval, auto_adjust=True)
            if not new_hist.empty:
                hist = pd.concat([hist, new_hist])
                print(f"ä¸‹è¼‰äº† {len(new_hist)} æ¢æ–°æ•¸æ“š", end='')
            else:
                print("æ²’æœ‰æ–°çš„æ•¸æ“šå¯ä¸‹è¼‰", end='')
    else:
        print(" - ç·©å­˜ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä¸‹è¼‰å…¨éƒ¨æ­·å²æ•¸æ“š...", end='')
        # æ ¹æ“š interval è¨­ç½®ä¸åŒçš„ period
        if interval == '1m':
            period = '7d'  # åˆ†é˜ç·šåªä¸‹è¼‰æœ€è¿‘7å¤©
        elif interval == '1h':
            period = '730d'  # å°æ™‚ç·šä¸‹è¼‰æœ€è¿‘2å¹´
        else:
            period = 'max'  # æ—¥ç·šä¸‹è¼‰å…¨éƒ¨æ­·å²
        hist = ticker.history(period=period, interval=interval, auto_adjust=True)
        print(f"ä¸‹è¼‰äº† {len(hist)} æ¢æ•¸æ“š", end='')

    # ä¼˜åŒ–å†…å­˜ä½¿ç”¨ - è½¬æ¢æ•°æ®ç±»å‹
    if not hist.empty:
        hist = optimize_dataframe_memory(hist)
        float_shares = info.get('floatShares', None)
        hist['FloatShares'] = float_shares
        hist.to_csv(csv_file)

    if info:
        # ä½¿ç”¨é€’å½’çš„ serialize_for_json å¤„ç†æ‰€æœ‰åµŒå¥—å±‚çº§
        processed_info = serialize_for_json(info)

        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(processed_info, f, ensure_ascii=False, indent=4)

    return hist, info, news

def run_analysis(market: str, force_fast_mode: bool = False, use_kronos: bool = True, symbol_filter: str = None, interval: str = '1d', max_workers: int = None):
    """
    å°æŒ‡å®šå¸‚å ´åŸ·è¡Œæ‰€æœ‰é¸è‚¡ç­–ç•¥åˆ†æ

    Args:
        market: å¸‚å ´ä»£ç¢¼ ('US' æˆ– 'HK')
        force_fast_mode: æ˜¯å¦å¼·åˆ¶è·³éç·©å­˜æ›´æ–°ï¼Œç›´æ¥ä½¿ç”¨å¿«é€Ÿæ¨¡å¼
        use_kronos: æ˜¯å¦ä½¿ç”¨ Kronos é æ¸¬ï¼ˆåƒ…é©ç”¨æ–¼æ¸¯è‚¡ï¼‰
        symbol_filter: æŒ‡å®šåˆ†æå–®ä¸€è‚¡ç¥¨ä»£ç¢¼ï¼ˆä¾‹å¦‚ï¼š0017.HKï¼‰
        interval: æ•¸æ“šæ™‚æ®µé¡å‹ ('1d' æ—¥ç·š, '1h' å°æ™‚ç·š, '1m' åˆ†é˜ç·š)
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
    """
    # åŠ è½½é…ç½®
    config = load_config()
    
    # å¦‚æœæœªæŒ‡å®šmax_workersï¼Œä»é…ç½®ä¸­è·å–
    if max_workers is None:
        max_workers = config['api']['max_workers']
    
    # --- å…¨å±€ç·©å­˜ç‰ˆæœ¬æª¢æŸ¥ ---
    version_file = os.path.join('data_cache', market.upper(), 'version.txt')
    today_str = datetime.now().date().isoformat()
    is_sync_needed = True

    if force_fast_mode:
        is_sync_needed = False
        print(f"--- å¼·åˆ¶å¿«é€Ÿæ¨¡å¼ï¼šè·³éç·©å­˜æ›´æ–°æª¢æŸ¥ ---")
    else:
        try:
            with open(version_file, 'r') as f:
                last_sync_date = f.read().strip()
            if last_sync_date == today_str:
                is_sync_needed = False
                print(f"--- æ•¸æ“šç·©å­˜å·²æ˜¯æœ€æ–° ({today_str})ï¼Œå°‡ä»¥å¿«é€Ÿæ¨¡å¼é‹è¡Œ ---")
            else:
                print(f"--- æ•¸æ“šç·©å­˜ä¸æ˜¯æœ€æ–° (ç‰ˆæœ¬: {last_sync_date})ï¼Œå°‡åŸ·è¡Œå¢é‡åŒæ­¥ ---")
        except FileNotFoundError:
            print("--- æœªæ‰¾åˆ°ç·©å­˜ç‰ˆæœ¬æ–‡ä»¶ï¼Œå°‡åŸ·è¡Œé¦–æ¬¡åŒæ­¥ ---")

    # --- ç²å–è‚¡ç¥¨åˆ—è¡¨ ---
    # å…ˆå®šç¾© market_ticker
    if market.upper() == 'US':
        market_ticker = '^GSPC'
    elif market.upper() == 'HK':
        market_ticker = '^HSI'
    else:
        print(f"éŒ¯èª¤: ä¸æ”¯æ´çš„å¸‚å ´ '{market}'ã€‚è«‹ä½¿ç”¨ 'US' æˆ– 'HK'ã€‚")
        return []

    if symbol_filter:
        # å¦‚æœæŒ‡å®šäº†å–®ä¸€è‚¡ç¥¨ï¼Œç›´æ¥ä½¿ç”¨è©²è‚¡ç¥¨
        tickers = [symbol_filter]
        print(f"--- ä½¿ç”¨æŒ‡å®šè‚¡ç¥¨: {symbol_filter} ---")
    else:
        # å¦å‰‡ç²å–æ•´å€‹å¸‚å ´çš„è‚¡ç¥¨åˆ—è¡¨
        if market.upper() == 'US':
            tickers = us_loader.get_us_tickers()
        elif market.upper() == 'HK':
            tickers = hk_loader.get_hk_tickers()

    is_market_healthy = False
    market_latest_return = 0.0
    try:
        market_hist = yf.Ticker(market_ticker).history(period='1y', auto_adjust=True)
        if not market_hist.empty and len(market_hist) >= 200:
            market_latest_return = market_hist['Close'].pct_change().iloc[-1] * 100
            market_hist['MA200'] = market_hist['Close'].rolling(window=200).mean()
            latest_market_data = market_hist.iloc[-1]
            is_market_healthy = latest_market_data['Close'] > latest_market_data['MA200']
            market_status_str = "å¤šé ­" if is_market_healthy else "ç©ºé ­"
            print(f"å·²æˆåŠŸç²å–å¤§ç›¤({market_ticker})æ•¸æ“šã€‚ä»Šæ—¥æ¼²è·Œ: {market_latest_return:.2f}%ã€‚å¸‚å ´è¶¨å‹¢: {market_status_str}")
        else:
            print(f"å¤§ç›¤æ­·å²æ•¸æ“šä¸è¶³ä»¥è¨ˆç®—200MA")
    except Exception as e:
        print(f"ç„¡æ³•ä¸‹è¼‰æˆ–åˆ†æå¤§ç›¤æ•¸æ“š ({market_ticker})ï¼Œç­–ç•¥ä¸­çš„å¤§ç›¤æ¿¾ç¶²å°‡ä¸æœƒå•Ÿç”¨ã€‚éŒ¯èª¤: {e}")

    strategies_to_run = get_strategies()
    if not strategies_to_run:
        print("è­¦å‘Š: åœ¨ 'strategies' æ–‡ä»¶å¤¾ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç­–ç•¥ã€‚")
        return []
    print(f"å·²åŠ è¼‰ {len(strategies_to_run)} å€‹ç­–ç•¥: {[s.name for s in strategies_to_run]}")

    # --- é€å€‹è‚¡ç¥¨é€²è¡Œåˆ†æå’Œé æ¸¬ ---
    print(f"\n--- é–‹å§‹é€å€‹è‚¡ç¥¨é€²è¡Œåˆ†æå’Œé æ¸¬ ---")
    qualified_stocks = []
    total_stocks = len(tickers)
    
    # å®æ—¶è¾“å‡ºç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨åˆ°æ–‡ä»¶
    realtime_output_enabled = config['analysis']['enable_realtime_output']
    if realtime_output_enabled:
        output_file = f"{datetime.now().strftime('%Y-%m-%d')}_{market.lower()}_qualified_stocks.txt"
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è‚¡ç¥¨
    def analyze_single_stock(symbol):
        """åˆ†æå•ä¸ªè‚¡ç¥¨çš„å‡½æ•°"""
        try:
            # è·å–è‚¡ç¥¨æ•¸æ“šï¼ˆæœƒè‡ªå‹•è™•ç†ç·©å­˜ï¼‰
            hist, info, news = get_data_with_cache(symbol, market, fast_mode=not is_sync_needed, interval=interval)
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            if hist.empty or len(hist) < 2 or info is None or (isinstance(info, dict) and len(info) == 0):
                return None, 0  # è¿”å›Noneè¡¨ç¤ºè¯¥è‚¡ç¥¨æœªé€šè¿‡ç­›é€‰ï¼Œ0è¡¨ç¤ºæœªåˆ†ææˆåŠŸ
            
            # æ•°æ®é¢„å¤„ç†ä¼˜åŒ–ï¼šåŸºç¡€ç­›é€‰
            config = load_config()
            enable_preprocessing = config['analysis']['enable_data_preprocessing']
            min_volume_threshold = config['analysis']['min_volume_threshold']
            
            if enable_preprocessing:
                # åŸºç¡€æ•°æ®è´¨é‡æ£€æŸ¥
                if 'Volume' in hist.columns and not hist['Volume'].empty:
                    recent_volume = hist['Volume'].tail(5).mean()  # æœ€è¿‘5å¤©å¹³å‡æˆäº¤é‡
                    if recent_volume < min_volume_threshold:
                        return None, 1  # æˆäº¤é‡è¿‡ä½ï¼Œè·³è¿‡åˆ†æï¼Œä½†è®¡å…¥å·²åˆ†æè®¡æ•°
                
                # æ£€æŸ¥ä»·æ ¼æ•°æ®æ˜¯å¦æœ‰æ•ˆ
                if 'Close' in hist.columns:
                    recent_prices = hist['Close'].tail(10)  # æœ€è¿‘10å¤©ä»·æ ¼
                    if recent_prices.isna().all() or (recent_prices <= 0).any():
                        return None, 1  # ä»·æ ¼æ•°æ®æ— æ•ˆï¼Œè·³è¿‡åˆ†æ
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆæ•°æ®ç‚¹
                if len(hist.dropna()) < 20:  # è‡³å°‘éœ€è¦20ä¸ªæœ‰æ•ˆæ•°æ®ç‚¹
                    return None, 1  # æ•°æ®ç‚¹ä¸è¶³ï¼Œè·³è¿‡åˆ†æ
            
            # åŸ·è¡Œæ‰€æœ‰ç­–ç•¥
            passed_strategies = []
            for strategy in strategies_to_run:
                if strategy.run(hist.copy(), info=info, market_return=market_latest_return, is_market_healthy=is_market_healthy):
                    passed_strategies.append(strategy.name)
            
            if passed_strategies:
                # æ­¥éª¤ 1: AI åˆ†æï¼ˆåœ¨ Kronos é¢„æµ‹ä¹‹å‰ï¼‰
                ai_analysis = None
                try:
                    ai_analysis = analyze_stock_with_ai({
                        'symbol': symbol,
                        'strategies': passed_strategies,
                        'info': info,
                        'market': market
                    }, hist, interval)
                except Exception as ai_e:
                    print(f" - AI åˆ†æå‡ºé”™: {ai_e}", end='')

                # æ­¥éª¤ 2: è°ƒç”¨ Kronos é¢„æµ‹ï¼ˆä»…æ¸¯è‚¡ä¸”å¯ç”¨ Kronosï¼‰
                kronos_prediction = "N/A"
                rise_prob = 0.0
                fall_prob = 0.0
                KRONOS_SCRIPT_PATH = "/Users/addison/Develop/yfinace/Kronos/scripts/prediction_hk.py"

                if market.upper() == 'HK' and use_kronos:
                    try:
                        command = ["python3", KRONOS_SCRIPT_PATH, symbol]
                        process = subprocess.run(
                            command,
                            capture_output=True,
                            text=True,
                            check=True,
                            timeout=300
                        )
                        kronos_prediction = process.stdout.strip()
                        # è§£æä¸Šå‡/ä¸‹è·Œæœºç‡
                        rise_prob, fall_prob = parse_kronos_prediction(kronos_prediction)
                    except subprocess.CalledProcessError as e:
                        error_output = e.stderr.strip()
                        kronos_prediction = f"é æ¸¬å¤±æ•—: {error_output}"
                    except subprocess.TimeoutExpired:
                        kronos_prediction = "é æ¸¬è¶…æ™‚"
                    except Exception as pred_e:
                        kronos_prediction = f"èª¿ç”¨å¤–éƒ¨è…³æœ¬æ™‚å‡ºéŒ¯: {pred_e}"

                # æ­¥éª¤ 3: ä»…å½“ä¸Šå‡æœºç‡ > ä¸‹è·Œæœºç‡æ—¶æ‰åŠ å…¥ qualified_stocksï¼ˆå¦‚æœå¯ç”¨äº† Kronosï¼‰
                # å¦‚æœæœªå¯ç”¨ Kronosï¼Œåˆ™ç›´æ¥åŠ å…¥ qualified_stocks
                if not use_kronos or rise_prob > fall_prob:
                    exchange = info.get('exchange', 'UNKNOWN')
                    stock_result = {
                        'symbol': symbol,
                        'exchange': exchange,
                        'strategies': passed_strategies,
                        'info': info,
                        'news': news,
                        'kronos_prediction': kronos_prediction,
                        'rise_prob': rise_prob,
                        'fall_prob': fall_prob,
                        'ai_analysis': ai_analysis
                    }
                    
                    # å®æ—¶è¾“å‡ºç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨
                    if realtime_output_enabled:
                        with threading.Lock():
                            with open(output_file, 'a', encoding='utf-8') as f:
                                f.write(f"{symbol} ç¬¦åˆç­–ç•¥: {passed_strategies}\n")
                                if ai_analysis:
                                    f.write(f"AI åˆ†æ: {ai_analysis['summary']}\n")
                                f.write("-" * 50 + "\n")
                    
                    if use_kronos:
                        print(f"\r{' ' * 80}\râœ… {symbol} ç¬¦åˆç­–ç•¥: {passed_strategies}, ä¸Šå‡æ©Ÿç‡: {rise_prob:.2f}% vs ä¸‹è·Œæ©Ÿç‡: {fall_prob:.2f}%")
                    else:
                        print(f"\r{' ' * 80}\râœ… {symbol} ç¬¦åˆç­–ç•¥: {passed_strategies}")
                    # è¾“å‡º AI åˆ†æç»“æœåˆ° console
                    if ai_analysis:
                        print(f"   ğŸ¤– AI åˆ†æ: {ai_analysis['summary']}")
                        print(f"   ğŸ¤– AI æ¨¡å‹: {ai_analysis['model_used']}")
                    else:
                        print(f"   ğŸ¤– AI åˆ†æ: æœªèƒ½å®Œæˆ")
                    return stock_result, 1
                else:
                    print(f"\r{' ' * 80}\râ­ï¸  {symbol} ç¬¦åˆç­–ç•¥ä½†ä¸Šå‡æ©Ÿç‡({rise_prob:.2f}%) â‰¤ ä¸‹è·Œæ©Ÿç‡({fall_prob:.2f}%)ï¼Œå·²è·³é")
                    return None, 1
            else:
                return None, 1  # è¿”å›Noneè¡¨ç¤ºè¯¥è‚¡ç¥¨æœªé€šè¿‡ç­–ç•¥ï¼Œä½†å·²åˆ†ææˆåŠŸ
        except Exception as e:
            print(f"\r{' ' * 80}\râŒ åˆ†æ {symbol} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None, 0  # è¿”å›0è¡¨ç¤ºåˆ†æå¤±è´¥

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ‰€æœ‰è‚¡ç¥¨
    analyzed_count = 0
    qualified_count = 0
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_symbol = {executor.submit(analyze_single_stock, symbol): symbol for symbol in tickers}
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_to_symbol):
            symbol = future_to_symbol[future]
            try:
                result, count = future.result()
                if result is not None:
                    qualified_stocks.append(result)
                    qualified_count += 1
                if count > 0:
                    analyzed_count += count
            except Exception as e:
                print(f"\r{' ' * 80}\râŒ è™•ç† {symbol} çš„çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            # è®¡ç®—é¢„ä¼°å®Œæˆæ—¶é—´
            elapsed_time = time.time() - start_time
            if analyzed_count > 0:
                avg_time_per_stock = elapsed_time / analyzed_count
                estimated_total_time = avg_time_per_stock * total_stocks
                remaining_time = estimated_total_time - elapsed_time
                remaining_minutes = max(0, int(remaining_time / 60))
            else:
                remaining_minutes = -1  # æœªå¼€å§‹è®¡ç®—
            
            # æ›´æ–°è¿›åº¦
            progress = analyzed_count / total_stocks
            if remaining_minutes >= 0:
                print(f"\råˆ†æé€²åº¦: [{int(progress * 20) * '#'}{int((1 - progress) * 20) * '-'}] {analyzed_count}/{total_stocks} å·²åˆ†æ, {qualified_count} ç¬¦åˆæ¢ä»¶, é ä¼°å‰©é¤˜: {remaining_minutes} åˆ†é˜", end='')
            else:
                print(f"\råˆ†æé€²åº¦: [{int(progress * 20) * '#'}{int((1 - progress) * 20) * '-'}] {analyzed_count}/{total_stocks} å·²åˆ†æ, {qualified_count} ç¬¦åˆæ¢ä»¶", end='')

    # --- æ›´æ–°ç·©å­˜ç‰ˆæœ¬æ–‡ä»¶ ---
    if is_sync_needed:
        print(f"\n--- æ›´æ–°ç·©å­˜ç‰ˆæœ¬è‡³ {today_str} ---")
        with open(version_file, 'w') as f:
            f.write(today_str)
    
    print(f"\n--- åˆ†æå®Œæˆï¼æˆåŠŸåˆ†æ {analyzed_count}/{total_stocks} æ”¯è‚¡ç¥¨ï¼Œæ‰¾åˆ° {len(qualified_stocks)} æ”¯ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨ ---")
    print(f"--- ç¸½è€—æ™‚: {int((time.time() - start_time) / 60)} åˆ†é˜ {int((time.time() - start_time) % 60)} ç§’ ---")
    return qualified_stocks