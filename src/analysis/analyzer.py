import yfinance as yf
import pandas as pd
import pkgutil
import importlib
import inspect
import os
import json
import subprocess
import re
import time
from datetime import datetime, timedelta, date
from src.strategies.base_strategy import BaseStrategy
from src.data_loader import us_loader, hk_loader
from src.ai.analyzer.ai_analyzer import analyze_stock_with_ai
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import numpy as np
import logging

# é…ç½®æ—¥å¿—
def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)
    
    log_filename = os.path.join(log_dir, f"analyzer_{datetime.now().strftime('%Y-%m-%d')}.log")
    
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
        ]
    )

# åˆå§‹åŒ–æ—¥å¿—
setup_logging()
logger = logging.getLogger(__name__)


def get_strategies():
    """
    å‹•æ…‹å¾ strategies æ¨¡çµ„åŠ è¼‰æ‰€æœ‰ç­–ç•¥é¡åˆ¥çš„å¯¦ä¾‹ã€‚
    """
    strategies = []
    import src.strategies as strategies_module
    strategy_path = strategies_module.__path__

    for _, name, _ in pkgutil.iter_modules(strategy_path):
        if name != 'base_strategy':
            module = importlib.import_module(f"src.strategies.{name}")
            for item_name, item in inspect.getmembers(module, inspect.isclass):
                if issubclass(item, BaseStrategy) and item is not BaseStrategy:
                    strategies.append(item())
    
    if not strategies:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç­–ç•¥ï¼Œç³»ç»Ÿå°†åªæ‰§è¡ŒAIåˆ†æ")
                
    return strategies

def _read_csv_with_auto_index(csv_file: str) -> pd.DataFrame:
    """
    è¯»å– CSV æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•åˆ—åï¼ˆDate æˆ– Datetimeï¼‰

    Args:
        csv_file: CSV æ–‡ä»¶è·¯å¾„

    Returns:
        DataFrame
    """
    # å…ˆè¯»å–ç¬¬ä¸€è¡Œæ¥æ£€æµ‹åˆ—å
    with open(csv_file, 'r') as f:
        first_line = f.readline()
    
    # æ£€æµ‹ç´¢å¼•åˆ—å
    if 'Datetime,' in first_line:
        index_col = 'Datetime'
    else:
        index_col = 'Date'
    
    # ä½¿ç”¨æ­£ç¡®çš„ç´¢å¼•åˆ—åè¯»å–
    return pd.read_csv(csv_file, index_col=index_col, parse_dates=True)

def validate_config(config):
    """
    éªŒè¯é…ç½®å€¼æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
    
    Args:
        config: é…ç½®å­—å…¸
        
    Raises:
        ValueError: å½“é…ç½®å€¼ä¸åœ¨åˆç†èŒƒå›´å†…æ—¶æŠ›å‡ºå¼‚å¸¸
    """
    errors = []
    
    # APIé…ç½®éªŒè¯
    api_config = config['api']
    if api_config['min_delay'] < 0:
        errors.append("min_delay ä¸èƒ½ä¸ºè´Ÿæ•°")
    if api_config['max_delay'] < api_config['min_delay']:
        errors.append("max_delay ä¸èƒ½å°äº min_delay")
    if api_config['base_delay'] < api_config['min_delay'] or api_config['base_delay'] > api_config['max_delay']:
        errors.append("base_delay åº”åœ¨ min_delay å’Œ max_delay ä¹‹é—´")
    if api_config['retry_attempts'] < 0:
        errors.append("retry_attempts ä¸èƒ½ä¸ºè´Ÿæ•°")
    if api_config['max_workers'] <= 0:
        errors.append("max_workers å¿…é¡»å¤§äº0")
    
    # æ•°æ®é…ç½®éªŒè¯
    if config['data']['max_cache_days'] <= 0:
        errors.append("max_cache_days å¿…é¡»å¤§äº0")
    
    # åˆ†æé…ç½®éªŒè¯
    if config['analysis']['min_volume_threshold'] < 0:
        errors.append("min_volume_threshold ä¸èƒ½ä¸ºè´Ÿæ•°")
    
    if errors:
        raise ValueError(f"é…ç½®éªŒè¯å¤±è´¥: {'; '.join(errors)}")
    
    return True

# é…ç½®ç¼“å­˜å˜é‡
_config_cache = None
_config_timestamp = None

def load_config(cached=True):
    """
    åŠ è½½é…ç½®æ–‡ä»¶
    
    Args:
        cached: æ˜¯å¦ä½¿ç”¨ç¼“å­˜çš„é…ç½®ï¼Œé»˜è®¤ä¸ºTrue
    """
    global _config_cache, _config_timestamp
    config_path = os.path.join(os.path.dirname(__file__), '..', 'config.json')
    
    # å¦‚æœå¯ç”¨ç¼“å­˜ä¸”ç¼“å­˜å­˜åœ¨ï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦è¢«ä¿®æ”¹
    if cached and _config_cache:
        try:
            import stat
            mtime = os.path.getmtime(config_path)
            if _config_timestamp and mtime <= _config_timestamp:
                return _config_cache
        except:
            pass  # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–å…¶ä»–é”™è¯¯ï¼Œç»§ç»­åŠ è½½
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
            
        # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„é…ç½®é¡¹éƒ½å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤å€¼
        default_config = {
            "api": {
                "base_delay": 0.5,
                "max_delay": 2.0,
                "min_delay": 0.1,
                "retry_attempts": 3,
                "max_workers": 4
            },
            "data": {
                "max_cache_days": 7,
                "float_dtype": "float32"
            },
            "analysis": {
                "enable_realtime_output": True,
                "enable_data_preprocessing": True,
                "min_volume_threshold": 100000
            }
        }
        
        # åˆå¹¶é…ç½®ï¼šä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å€¼ï¼Œå¯¹äºç¼ºå¤±çš„é…ç½®ä½¿ç”¨é»˜è®¤å€¼
        for section, section_data in default_config.items():
            if section not in config:
                config[section] = {}
            for key, value in section_data.items():
                if key not in config[section]:
                    config[section][key] = value
        
        # éªŒè¯é…ç½®
        validate_config(config)
        
        # æ›´æ–°ç¼“å­˜
        _config_cache = config
        _config_timestamp = os.path.getmtime(config_path) if os.path.exists(config_path) else None
        
        return config
    except FileNotFoundError:
        print(f"é…ç½®æ–‡ä»¶ {config_path} æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        default_config = {
            "api": {
                "base_delay": 0.5,
                "max_delay": 2.0,
                "min_delay": 0.1,
                "retry_attempts": 3,
                "max_workers": 4
            },
            "data": {
                "max_cache_days": 7,
                "float_dtype": "float32"
            },
            "analysis": {
                "enable_realtime_output": True,
                "enable_data_preprocessing": True,
                "min_volume_threshold": 100000
            }
        }
        
        # éªŒè¯é»˜è®¤é…ç½®
        validate_config(default_config)
        
        # æ›´æ–°ç¼“å­˜
        _config_cache = default_config
        _config_timestamp = None
        
        return default_config

def optimize_dataframe_memory(df: pd.DataFrame) -> pd.DataFrame:
    """
    ä¼˜åŒ– DataFrame çš„å†…å­˜ä½¿ç”¨ï¼Œé€šè¿‡ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç±»å‹

    Args:
        df: åŸå§‹ DataFrame

    Returns:
        ä¼˜åŒ–åçš„ DataFrame
    """
    df_optimized = df.copy()
    
    for col in df_optimized.columns:
        col_type = df_optimized[col].dtype
        
        if col_type != 'object':
            c_min = df_optimized[col].min()
            c_max = df_optimized[col].max()
            
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df_optimized[col] = df_optimized[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df_optimized[col] = df_optimized[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df_optimized[col] = df_optimized[col].astype(np.int32)
                else:
                    df_optimized[col] = df_optimized[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df_optimized[col] = df_optimized[col].astype(np.float32)
                else:
                    df_optimized[col] = df_optimized[col].astype(np.float64)
    
    return df_optimized

def serialize_for_json(obj):
    """
    å°†å¯¹è±¡è½¬æ¢ä¸ºå¯ JSON åºåˆ—åŒ–çš„æ ¼å¼ï¼ˆé€’å½’å¤„ç†æ‰€æœ‰å±‚çº§ï¼‰

    Args:
        obj: è¦åºåˆ—åŒ–çš„å¯¹è±¡

    Returns:
        å¯ JSON åºåˆ—åŒ–çš„å¯¹è±¡
    """
    import pandas as pd
    import numpy as np
    from datetime import datetime, date

    # é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸º NaNï¼ˆæ ‡é‡ï¼‰
    try:
        if isinstance(obj, (float, int)) and np.isnan(obj):
            return None
    except (TypeError, ValueError):
        pass

    # å¤„ç†æ—¥æœŸæ—¶é—´å¯¹è±¡
    if isinstance(obj, (pd.Timestamp, datetime, date)):
        return obj.isoformat()

    # å¤„ç† Series æˆ– DataFrame - å…ˆè½¬æ¢ä¸ºå­—å…¸ï¼Œç„¶åé€’å½’å¤„ç†
    elif isinstance(obj, (pd.Series, pd.DataFrame)):
        result_dict = obj.to_dict()
        return serialize_for_json(result_dict)  # å…³é”®ï¼šé€’å½’å¤„ç†è½¬æ¢åçš„å­—å…¸

    # å¤„ç†å­—å…¸ - é€’å½’å¤„ç†é”®å’Œå€¼
    elif isinstance(obj, dict):
        result = {}
        for k, v in obj.items():
            # å¤„ç†é”®
            if isinstance(k, (pd.Timestamp, datetime, date)):
                k = k.isoformat()
            # é€’å½’å¤„ç†å€¼
            result[k] = serialize_for_json(v)
        return result

    # å¤„ç†åˆ—è¡¨/å…ƒç»„ - é€’å½’å¤„ç†æ¯ä¸ªå…ƒç´ 
    elif isinstance(obj, (list, tuple)):
        return [serialize_for_json(item) for item in obj]

    # å¤„ç† numpy ç±»å‹
    elif isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()

    # å…¶ä»–ç±»å‹ç›´æ¥è¿”å›
    else:
        return obj

def get_enhanced_financial_data(ticker: yf.Ticker) -> dict:
    """
    è·å–å¢å¼ºçš„è´¢åŠ¡æ•°æ®ï¼ŒåŒ…æ‹¬è´¢åŠ¡æŠ¥è¡¨å’Œå…³é”®æŒ‡æ ‡

    Args:
        ticker: yfinance Ticker å¯¹è±¡

    Returns:
        åŒ…å«å¢å¼ºè´¢åŠ¡æ•°æ®çš„å­—å…¸
    """
    enhanced_data = {}

    try:
        # åªè·å–æœ€å…³é”®çš„è´¢åŠ¡æŠ¥è¡¨æ•°æ®ï¼Œå‡å°‘ API è°ƒç”¨
        financials = ticker.financials
        if financials is not None and isinstance(financials, pd.DataFrame) and not financials.empty:
            enhanced_data['financials'] = financials.to_dict()

        # è·å–èµ„äº§è´Ÿå€ºè¡¨
        balance_sheet = ticker.balance_sheet
        if balance_sheet is not None and isinstance(balance_sheet, pd.DataFrame) and not balance_sheet.empty:
            enhanced_data['balance_sheet'] = balance_sheet.to_dict()

        # è·å–ç°é‡‘æµé‡è¡¨
        cashflow = ticker.cashflow
        if cashflow is not None and isinstance(cashflow, pd.DataFrame) and not cashflow.empty:
            enhanced_data['cashflow'] = cashflow.to_dict()

    except Exception as e:
        print(f" - [å¢å¼ºæ•°æ®] è·å–å¤±è´¥: {e}", end='')

    return enhanced_data

class APIDelayer:
    """
    APIå»¶è¿Ÿç®¡ç†ç±»ï¼Œå®ç°æ™ºèƒ½å»¶è¿Ÿç­–ç•¥
    """
    def __init__(self, config):
        self.config = config['api']
        self.last_response_time = time.time()
        self.failure_count = 0
        self.successful_requests = 0
        self.total_delay = 0

    def calculate_delay(self, is_failure=False):
        """
        è®¡ç®—APIå»¶è¿Ÿæ—¶é—´
        
        Args:
            is_failure: æ˜¯å¦æ˜¯å¤±è´¥è¯·æ±‚ï¼Œå¦‚æœæ˜¯åˆ™å¢åŠ å»¶è¿Ÿ
        """
        base_delay = self.config['base_delay']
        
        # åŸºäºå¤±è´¥æ¬¡æ•°çš„æŒ‡æ•°é€€é¿
        if is_failure:
            self.failure_count += 1
            current_delay = base_delay * (1.5 ** min(self.failure_count, 5))  # æœ€å¤§é€€é¿5æ¬¡
        else:
            # æˆåŠŸè¯·æ±‚æ—¶å‡å°‘å¤±è´¥è®¡æ•°å™¨ï¼ˆä½†ä¸é‡ç½®ï¼‰
            if self.failure_count > 0:
                self.failure_count = max(0, self.failure_count - 0.1)
            current_delay = base_delay * (0.95 ** min(self.successful_requests, 10))  # åˆå§‹æˆåŠŸæ—¶å¯ç•¥å¾®é™ä½å»¶è¿Ÿ

        # åº”ç”¨æœ€å°å’Œæœ€å¤§å»¶è¿Ÿé™åˆ¶
        current_delay = max(self.config['min_delay'], 
                           min(current_delay, self.config['max_delay']))
        
        return current_delay

    def apply_delay(self, is_failure=False):
        """
        åº”ç”¨APIå»¶è¿Ÿ
        
        Args:
            is_failure: æ˜¯å¦æ˜¯å¤±è´¥è¯·æ±‚ï¼Œå¦‚æœæ˜¯åˆ™å¢åŠ å»¶è¿Ÿ
        """
        delay = self.calculate_delay(is_failure)
        time.sleep(delay)
        self.total_delay += delay

    def record_request_result(self, is_success):
        """
        è®°å½•è¯·æ±‚ç»“æœä»¥è°ƒæ•´å»¶è¿Ÿç­–ç•¥
        
        Args:
            is_success: è¯·æ±‚æ˜¯å¦æˆåŠŸ
        """
        if is_success:
            self.successful_requests += 1
        else:
            self.failure_count += 1


def calculate_technical_indicators(hist: pd.DataFrame) -> pd.DataFrame:
    """
    é¢„è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å¹¶æ·»åŠ åˆ°å†å²æ•°æ®ä¸­
    
    Args:
        hist: åŒ…å«OHLCVæ•°æ®çš„DataFrame
        
    Returns:
        æ·»åŠ äº†æŠ€æœ¯æŒ‡æ ‡çš„DataFrame
    """
    if hist is None or hist.empty or 'Close' not in hist.columns:
        return hist
    
    # å¤åˆ¶æ•°æ®ä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
    result = hist.copy()
    
    try:
        # RSI (14)
        delta = result['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14, min_periods=1).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=1).mean()
        rs = gain / loss
        result['RSI_14'] = 100 - (100 / (1 + rs))
        
        # MACD (12, 26, 9)
        exp12 = result['Close'].ewm(span=12, adjust=False).mean()
        exp26 = result['Close'].ewm(span=26, adjust=False).mean()
        macd = exp12 - exp26
        signal = macd.ewm(span=9, adjust=False).mean()
        result['MACD'] = macd
        result['MACD_Signal'] = signal
        result['MACD_Histogram'] = macd - signal
        
        # ATR (14)
        high_low = result['High'] - result['Low']
        high_close = abs(result['High'] - result['Close'].shift())
        low_close = abs(result['Low'] - result['Close'].shift())
        tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)
        result['ATR_14'] = tr.rolling(window=14, min_periods=1).mean()
        
        # å¸ƒæ—å¸¦ (20, 2)
        sma20 = result['Close'].rolling(window=20, min_periods=1).mean()
        std20 = result['Close'].rolling(window=20, min_periods=1).std()
        result['BB_Middle'] = sma20
        result['BB_Upper'] = sma20 + (std20 * 2)
        result['BB_Lower'] = sma20 - (std20 * 2)
        
        # ç§»åŠ¨å¹³å‡çº¿
        result['MA_5'] = result['Close'].rolling(window=5, min_periods=1).mean()
        result['MA_10'] = result['Close'].rolling(window=10, min_periods=1).mean()
        result['MA_20'] = result['Close'].rolling(window=20, min_periods=1).mean()
        result['MA_50'] = result['Close'].rolling(window=50, min_periods=1).mean()
        result['MA_200'] = result['Close'].rolling(window=200, min_periods=1).mean()
        
        # æˆäº¤é‡ç§»åŠ¨å¹³å‡
        result['Volume_MA_20'] = result['Volume'].rolling(window=20, min_periods=1).mean()
        
        # ä»·æ ¼å˜åŒ–ç‡
        result['Price_Change_Pct'] = result['Close'].pct_change(fill_method=None)
        result['Price_Change_Pct_5D'] = result['Close'].pct_change(periods=5, fill_method=None)
        
    except Exception as e:
        print(f" - [æŠ€æœ¯æŒ‡æ ‡è®¡ç®—] è®¡ç®—æŠ€æœ¯æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
        # å¦‚æœè®¡ç®—å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®
        return hist
    
    return result


def get_data_with_cache(symbol: str, market: str, fast_mode: bool = False, interval: str = '1d', config=None) -> (pd.DataFrame, dict, dict):
    """
    ç²å–è‚¡ç¥¨æ•¸æ“šï¼Œæ ¹æ“šæ¨¡å¼é¸æ“‡å¿«é€ŸåŠ è¼‰æˆ–åŒæ­¥æ›´æ–°ã€‚

    Args:
        symbol: è‚¡ç¥¨ä»£ç¢¼
        market: å¸‚å ´ä»£ç¢¼ ('US' æˆ– 'HK')
        fast_mode: æ˜¯å¦ä½¿ç”¨å¿«é€Ÿæ¨¡å¼
        interval: æ•¸æ“šæ™‚æ®µé¡å‹ ('1d' æ—¥ç·š, '1h' å°æ™‚ç·š, '1m' åˆ†é˜ç·š)
        config: é…ç½®å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™åŠ è½½é…ç½®
    """
    cache_dir = os.path.join('data_cache', market.upper())
    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
    os.makedirs(cache_dir, exist_ok=True)
    safe_symbol = symbol.replace(":", "_")
    csv_file = os.path.join(cache_dir, f"{safe_symbol}_{interval}.csv")  # æ·»åŠ  interval åˆ°æ–‡ä»¶å
    json_file = os.path.join(cache_dir, f"{safe_symbol}.json")

    # è·å–é…ç½®
    if config is None:
        config = load_config()
    
    api_config = config['api']
    retry_attempts = api_config['retry_attempts']

    ticker = yf.Ticker(symbol)

    if fast_mode:
        try:
            # è‡ªåŠ¨æ£€æµ‹ç´¢å¼•åˆ—åï¼ˆDate æˆ– Datetimeï¼‰
            hist = _read_csv_with_auto_index(csv_file)
            with open(json_file, 'r', encoding='utf-8') as f:
                info = json.load(f)

            # ç¡®ä¿ info æ˜¯å­—å…¸
            if not isinstance(info, dict):
                info = {}

            # éªŒè¯å…³é”®å­—æ®µ
            required_fields = [
                'marketCap', 'trailingPE', 'forwardPE', 'pegRatio', 'priceToBook',
                'profitMargins', 'returnOnEquity', 'revenueGrowth', 'earningsGrowth',
                'dividendYield', 'beta', '52WeekChange', 'targetMeanPrice',
                'volume', 'floatShares', 'shortRatio'
            ]
            for field in required_fields:
                if field not in info:
                    info[field] = None

            # ç§»é™¤ news è°ƒç”¨ä»¥å‡å°‘ API è¯·æ±‚
            news = []
            # ä¼˜åŒ–å†…å­˜ä½¿ç”¨ - è½¬æ¢æ•°æ®ç±»å‹
            hist = optimize_dataframe_memory(hist)
            # é¢„è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            hist = calculate_technical_indicators(hist, config)
            logger.info(f"å¿«é€Ÿæ¨¡å¼åŠ è½½ {symbol} æ•°æ®æˆåŠŸ: {len(hist)} æ¡è®°å½•")
            return hist, info, news
        except FileNotFoundError:
            logger.info(f"å¿«é€Ÿæ¨¡å¼ - ç¼“å­˜æ–‡ä»¶æœªæ‰¾åˆ°: {csv_file} æˆ– {json_file}")
            return get_data_with_cache(symbol, market, fast_mode=False, interval=interval, config=config)
        except (json.JSONDecodeError, ValueError) as e:
            logger.error(f"å¿«é€Ÿæ¨¡å¼ - JSON è§£æå¤±è´¥ {symbol}: {e}")
            # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
            try:
                os.remove(json_file)
                logger.info(f"å·²åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶: {json_file}")
            except Exception as rm_e:
                logger.warning(f"åˆ é™¤æŸåç¼“å­˜æ–‡ä»¶å¤±è´¥: {rm_e}")
            return get_data_with_cache(symbol, market, fast_mode=False, interval=interval, config=config)
        except Exception as e:
            logger.error(f"å¿«é€Ÿæ¨¡å¼ - åŠ è½½ {symbol} æ•°æ®æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return get_data_with_cache(symbol, market, fast_mode=False, interval=interval, config=config)

    # --- æ­£å¸¸åŒæ­¥æ¨¡å¼ ---
    today = datetime.now().date()
    hist, info, news = pd.DataFrame(), {}, []

    # åˆ›å»ºAPIå»¶è¿Ÿç®¡ç†å™¨
    delayer = APIDelayer(config)

    # é¦–å…ˆè·å–å†å²ä»·æ ¼æ•°æ®ï¼ˆè¿™ä¸ªé€šå¸¸æ¯” info æ›´å®¹æ˜“è·å–ï¼‰
    if os.path.exists(csv_file):
        try:
            # è‡ªåŠ¨æ£€æµ‹ç´¢å¼•åˆ—åï¼ˆDate æˆ– Datetimeï¼‰
            hist = _read_csv_with_auto_index(csv_file)
            last_cached_date = hist.index.max().date()

            if last_cached_date >= today:
                logger.info(f"ä»ç¼“å­˜åŠ è½½ {symbol} {len(hist)} æ¡æ•°æ®")
            else:
                start_date = last_cached_date + timedelta(days=1)
                logger.info(f"ç¼“å­˜æ•°æ®è¿‡æ—§ï¼Œæ­£åœ¨ä» {start_date.strftime('%Y-%m-%d')} ä¸‹è½½å¢é‡æ•°æ®...")
                
                # åº”ç”¨APIå»¶è¿Ÿ
                delayer.apply_delay()
                
                new_hist = ticker.history(start=start_date.strftime('%Y-%m-%d'), interval=interval, auto_adjust=True)
                if not new_hist.empty:
                    hist = pd.concat([hist, new_hist])
                    logger.info(f"ä¸‹è½½äº† {len(new_hist)} æ¡æ–°æ•°æ®")
                else:
                    logger.info("æ²¡æœ‰æ–°çš„æ•°æ®å¯ä¸‹è½½")
        except Exception as e:
            logger.warning(f"åŠ è½½å†å²ç¼“å­˜æ•°æ®å¤±è´¥ï¼Œå°†é‡æ–°ä¸‹è½½: {e}")
            hist = pd.DataFrame()  # é‡ç½®ä¸ºæ–°çš„æ•°æ®

    if hist.empty or hist.shape[0] == 0:
        logger.info(f"ç¼“å­˜ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥ï¼Œæ­£åœ¨ä¸‹è½½ {symbol} å…¨éƒ¨å†å²æ•°æ®...")
        # æ ¹æ“š interval è¨­ç½®ä¸åŒçš„ period
        if interval == '1m':
            period = '7d'  # åˆ†é˜ç·šåªä¸‹è¼‰æœ€è¿‘7å¤©
        elif interval == '1h':
            period = '730d'  # å°æ™‚ç·šä¸‹è¼‰æœ€è¿‘2å¹´
        else:
            period = 'max'  # æ—¥ç·šä¸‹è¼‰å…¨éƒ¨æ­·å²
        hist = ticker.history(period=period, interval=interval, auto_adjust=True)
        logger.info(f"ä¸‹è½½äº† {len(hist)} æ¡æ•°æ®")

    # é¢„è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
    hist = calculate_technical_indicators(hist, config)
    
    # ä¼˜åŒ–å†…å­˜ä½¿ç”¨ - è½¬æ¢æ•°æ®ç±»å‹
    if not hist.empty:
        hist = optimize_dataframe_memory(hist)
        float_shares = None  # æš‚æ—¶è®¾ç½®ä¸º None
        hist['FloatShares'] = float_shares
        hist.to_csv(csv_file)

    # å°è¯•è·å– info æ•°æ®ï¼ˆå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨ç©ºå­—å…¸ï¼‰
    info_loaded = False
    for attempt in range(retry_attempts):
        try:
            logger.debug(f"å°è¯•è·å– {symbol} info æ•°æ® (ç¬¬ {attempt + 1}/{retry_attempts} æ¬¡)")
            # åº”ç”¨APIå»¶è¿Ÿ
            delayer.apply_delay()
            
            info = ticker.info
            # ç¡®ä¿ info ä¸ä¸ºç©º - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼
            if info is None:
                logger.warning(f"{symbol} info æ•°æ®ä¸ºç©º")
                info = {}
            elif not isinstance(info, dict):
                # å¦‚æœ info ä¸æ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºå­—å…¸
                logger.warning(f"{symbol} info æ ¼å¼å¼‚å¸¸ï¼Œè½¬æ¢ä¸ºå­—å…¸")
                info = {}
            elif isinstance(info, dict) and len(info) == 0:
                logger.warning(f"{symbol} info å­—å…¸ä¸ºç©º")
                # ä¿æŒä¸ºç©ºå­—å…¸ï¼Œç»§ç»­å°è¯•è·å–å¢å¼ºæ•°æ®

            # éªŒè¯å…³é”®å­—æ®µæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¾ç½®ä¸º None
            required_fields = [
                'marketCap', 'trailingPE', 'forwardPE', 'pegRatio', 'priceToBook',
                'profitMargins', 'returnOnEquity', 'revenueGrowth', 'earningsGrowth',
                'dividendYield', 'beta', '52WeekChange', 'targetMeanPrice',
                'volume', 'floatShares', 'shortRatio'
            ]
            for field in required_fields:
                if field not in info:
                    info[field] = None

            # è·å–å¢å¼ºçš„è´¢åŠ¡æ•°æ®
            enhanced_data = get_enhanced_financial_data(ticker)
            if enhanced_data:
                info['enhanced_financial_data'] = enhanced_data

            # ä¿å­˜ info åˆ°ç¼“å­˜ - åªåœ¨æœ‰æœ‰æ•ˆæ•°æ®æ—¶ä¿å­˜
            if isinstance(info, dict) and len(info) > 0:
                try:
                    # ä½¿ç”¨é€’å½’çš„ serialize_for_json å¤„ç†æ‰€æœ‰åµŒå¥—å±‚çº§
                    processed_info = serialize_for_json(info)

                    with open(json_file, 'w', encoding='utf-8') as f:
                        json.dump(processed_info, f, ensure_ascii=False, indent=4)
                    logger.debug(f"æˆåŠŸä¿å­˜ {symbol} info åˆ°ç¼“å­˜")
                except Exception as save_error:
                    logger.error(f"ä¿å­˜ {symbol} info å¤±è´¥: {save_error}")
                    # ä¿å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹

            # è®°å½•æˆåŠŸè¯·æ±‚
            delayer.record_request_result(True)
            info_loaded = True
            break  # æˆåŠŸè·å–infoï¼Œè·³å‡ºé‡è¯•å¾ªç¯
        except Exception as e:
            logger.error(f"æ— æ³•è·å– {symbol} info (å°è¯• {attempt + 1}/{retry_attempts}): {e}")
            # è®°å½•å¤±è´¥è¯·æ±‚
            delayer.record_request_result(False)
            if attempt < retry_attempts - 1:
                logger.debug(f"é‡è¯•å‰ç­‰å¾…: ç¬¬ {attempt + 1} æ¬¡")
                delayer.apply_delay(is_failure=True)  # é‡è¯•å‰ç­‰å¾…æ›´é•¿æ—¶é—´
            else:
                logger.error(f"{symbol} æ‰€æœ‰é‡è¯•å‡å¤±è´¥ï¼Œå°†ä½¿ç”¨ç©ºæ•°æ®")
                info = {}

    if not info_loaded:
        logger.warning(f"æœªèƒ½è·å– {symbol} çš„ info æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤ç©ºæ•°æ®")

    news = []

    # æœ€åå†æ£€æŸ¥ä¸€æ¬¡ç¼“å­˜ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
    if os.path.exists(csv_file) and hist.empty:
        try:
            # è‡ªåŠ¨æ£€æµ‹ç´¢å¼•åˆ—åï¼ˆDate æˆ– Datetimeï¼‰
            hist = _read_csv_with_auto_index(csv_file)
            logger.info(f"ä»CSVæ–‡ä»¶é‡æ–°åŠ è½½ {symbol} çš„å†å²æ•°æ®: {len(hist)} æ¡")
        except Exception as e:
            logger.error(f"é‡æ–°ä»CSVåŠ è½½ {symbol} æ•°æ®å¤±è´¥: {e}")

    # é¢„è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆå¦‚æœå°šæœªè®¡ç®—ï¼‰
    if not hist.empty and 'RSI_14' not in hist.columns:
        hist = calculate_technical_indicators(hist, config)
    
    # ä¼˜åŒ–å†…å­˜ä½¿ç”¨ - è½¬æ¢æ•°æ®ç±»å‹
    if not hist.empty:
        hist = optimize_dataframe_memory(hist)
        float_shares = info.get('floatShares', None)
        hist['FloatShares'] = float_shares
        hist.to_csv(csv_file)

    if info:
        try:
            # ä½¿ç”¨é€’å½’çš„ serialize_for_json å¤„ç†æ‰€æœ‰åµŒå¥—å±‚çº§
            processed_info = serialize_for_json(info)

            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(processed_info, f, ensure_ascii=False, indent=4)
        except Exception as e:
            logger.error(f"ä¿å­˜ {symbol} JSON ç¼“å­˜å¤±è´¥: {e}")

    logger.info(f"æˆåŠŸè·å– {symbol} æ•°æ®: {len(hist)} æ¡è®°å½•, infoå­—æ®µæ•°: {len(info) if info else 0}")
    return hist, info, news

def run_analysis(market: str, force_fast_mode: bool = False, skip_strategies: bool = False, symbol_filter: str = None, interval: str = '1d', max_workers: int = None, model: str = 'iflow-rome-30ba3b'):
    """
    å°æŒ‡å®šå¸‚å ´åŸ·è¡Œæ‰€æœ‰é¸è‚¡ç­–ç•¥åˆ†æ

    Args:
        market: å¸‚å ´ä»£ç¢¼ ('US' æˆ– 'HK')
        force_fast_mode: æ˜¯å¦å¼·åˆ¶è·³éç·©å­˜æ›´æ–°ï¼Œç›´æ¥ä½¿ç”¨å¿«é€Ÿæ¨¡å¼
        symbol_filter: æŒ‡å®šåˆ†æå–®ä¸€è‚¡ç¥¨ä»£ç¢¼ï¼ˆä¾‹å¦‚ï¼š0017.HKï¼‰
        interval: æ•¸æ“šæ™‚æ®µé¡å‹ ('1d' æ—¥ç·š, '1h' å°æ™‚ç·š, '1m' åˆ†é˜ç·š)
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼Œé»˜è®¤ä¸ºNoneï¼ˆä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰
    """
    # åŠ è½½é…ç½®
    config = load_config()
    
    # å¦‚æœæœªæŒ‡å®šmax_workersï¼Œä»é…ç½®ä¸­è·å–
    if max_workers is None:
        max_workers = config['api']['max_workers']
    
    # --- å…¨å±€ç·©å­˜ç‰ˆæœ¬æª¢æŸ¥ ---
    version_file = os.path.join('data_cache', market.upper(), 'version.txt')
    today_str = datetime.now().date().isoformat()
    is_sync_needed = True

    if force_fast_mode:
        is_sync_needed = False
        print(f"--- å¼·åˆ¶å¿«é€Ÿæ¨¡å¼ï¼šè·³éç·©å­˜æ›´æ–°æª¢æŸ¥ ---")
    else:
        try:
            with open(version_file, 'r') as f:
                last_sync_date = f.read().strip()
            if last_sync_date == today_str:
                is_sync_needed = False
                print(f"--- æ•¸æ“šç·©å­˜å·²æ˜¯æœ€æ–° ({today_str})ï¼Œå°‡ä»¥å¿«é€Ÿæ¨¡å¼é‹è¡Œ ---")
            else:
                print(f"--- æ•¸æ“šç·©å­˜ä¸æ˜¯æœ€æ–° (ç‰ˆæœ¬: {last_sync_date})ï¼Œå°‡åŸ·è¡Œå¢é‡åŒæ­¥ ---")
        except FileNotFoundError:
            print("--- æœªæ‰¾åˆ°ç·©å­˜ç‰ˆæœ¬æ–‡ä»¶ï¼Œå°‡åŸ·è¡Œé¦–æ¬¡åŒæ­¥ ---")

    # --- ç²å–è‚¡ç¥¨åˆ—è¡¨ ---
    # å…ˆå®šç¾© market_ticker
    if market.upper() == 'US':
        market_ticker = '^GSPC'
    elif market.upper() == 'HK':
        market_ticker = '^HSI'
    else:
        print(f"éŒ¯èª¤: ä¸æ”¯æ´çš„å¸‚å ´ '{market}'ã€‚è«‹ä½¿ç”¨ 'US' æˆ– 'HK'ã€‚")
        return []

    if symbol_filter:
        # å¦‚æœæŒ‡å®šäº†å–®ä¸€è‚¡ç¥¨ï¼Œç›´æ¥ä½¿ç”¨è©²è‚¡ç¥¨
        tickers = [symbol_filter]
        print(f"--- ä½¿ç”¨æŒ‡å®šè‚¡ç¥¨: {symbol_filter} ---")
    else:
        # å¦å‰‡ç²å–æ•´å€‹å¸‚å ´çš„è‚¡ç¥¨åˆ—è¡¨
        if market.upper() == 'US':
            tickers = us_loader.get_us_tickers()
        elif market.upper() == 'HK':
            tickers = hk_loader.get_hk_tickers()

    is_market_healthy = False
    market_latest_return = 0.0
    try:
        market_hist = yf.Ticker(market_ticker).history(period='1y', auto_adjust=True)
        if not market_hist.empty and len(market_hist) >= 200:
            market_latest_return = market_hist['Close'].pct_change().iloc[-1] * 100
            market_hist['MA200'] = market_hist['Close'].rolling(window=200).mean()
            latest_market_data = market_hist.iloc[-1]
            is_market_healthy = latest_market_data['Close'] > latest_market_data['MA200']
            market_status_str = "å¤šé ­" if is_market_healthy else "ç©ºé ­"
            print(f"å·²æˆåŠŸç²å–å¤§ç›¤({market_ticker})æ•¸æ“šã€‚ä»Šæ—¥æ¼²è·Œ: {market_latest_return:.2f}%ã€‚å¸‚å ´è¶¨å‹¢: {market_status_str}")
        else:
            print(f"å¤§ç›¤æ­·å²æ•¸æ“šä¸è¶³ä»¥è¨ˆç®—200MA")
    except Exception as e:
        print(f"ç„¡æ³•ä¸‹è¼‰æˆ–åˆ†æå¤§ç›¤æ•¸æ“š ({market_ticker})ï¼Œç­–ç•¥ä¸­çš„å¤§ç›¤æ¿¾ç¶²å°‡ä¸æœƒå•Ÿç”¨ã€‚éŒ¯èª¤: {e}")

    strategies_to_run = get_strategies()
    if not strategies_to_run:
        print("è­¦å‘Š: åœ¨ 'strategies' æ–‡ä»¶å¤¾ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½•ç­–ç•¥ã€‚")
        return []
    print(f"å·²åŠ è¼‰ {len(strategies_to_run)} å€‹ç­–ç•¥: {[s.name for s in strategies_to_run]}")

    # --- é€å€‹è‚¡ç¥¨é€²è¡Œåˆ†æå’Œé æ¸¬ ---
    print(f"\n--- é–‹å§‹é€å€‹è‚¡ç¥¨é€²è¡Œåˆ†æå’Œé æ¸¬ ---")
    qualified_stocks = []
    total_stocks = len(tickers)
    
    # å®æ—¶è¾“å‡ºç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨åˆ°ä¸»æŠ¥å‘Šæ–‡ä»¶
    realtime_output_enabled = config['analysis']['enable_realtime_output']
    main_report_file = f"{market.lower()}_stocks_{datetime.now().strftime('%Y-%m-%d')}.txt"
    if realtime_output_enabled:
        print(f"--- å¯¦æ™‚è¼¸å‡ºå·²å•Ÿç”¨ï¼Œå°‡è¨˜éŒ„åˆ°ä¸»å ±å‘Šæ–‡ä»¶: {main_report_file} ---")
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è‚¡ç¥¨
    def analyze_single_stock(symbol, config, skip_strategies=False, model='iflow-rome-30ba3b'):
        """åˆ†æå•ä¸ªè‚¡ç¥¨çš„å‡½æ•°ï¼Œæ¥å—é…ç½®å‚æ•°
        
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
            config: é…ç½®å¯¹è±¡
            skip_strategies: æ˜¯å¦è·³è¿‡ç­–ç•¥ç­›é€‰ï¼Œæ‰€æœ‰è‚¡ç¥¨éƒ½è¿›è¡ŒAIåˆ†æ
            model: è¦ä½¿ç”¨çš„AIæ¨¡å‹åç§°
        """
        try:
            # è·å–è‚¡ç¥¨æ•¸æ“šï¼ˆæœƒè‡ªå‹•è™•ç†ç·©å­˜ï¼‰
            hist, info, news = get_data_with_cache(symbol, market, fast_mode=not is_sync_needed, interval=interval, config=config)
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            if hist.empty or len(hist) < 2 or info is None or (isinstance(info, dict) and len(info) == 0):
                return None, 0  # è¿”å›Noneè¡¨ç¤ºè¯¥è‚¡ç¥¨æœªé€šè¿‡ç­›é€‰ï¼Œ0è¡¨ç¤ºæœªåˆ†ææˆåŠŸ
            
            # æ•°æ®é¢„å¤„ç†ä¼˜åŒ–ï¼šåŸºç¡€ç­›é€‰
            enable_preprocessing = config['analysis']['enable_data_preprocessing']
            min_volume_threshold = config['analysis']['min_volume_threshold']
            
            if enable_preprocessing:
                # åŸºç¡€æ•°æ®è´¨é‡æ£€æŸ¥
                if 'Volume' in hist.columns and not hist['Volume'].empty:
                    recent_volume = hist['Volume'].tail(5).mean()  # æœ€è¿‘5å¤©å¹³å‡æˆäº¤é‡
                    if recent_volume < min_volume_threshold:
                        return None, 1  # æˆäº¤é‡è¿‡ä½ï¼Œè·³è¿‡åˆ†æï¼Œä½†è®¡å…¥å·²åˆ†æè®¡æ•°
                
                # æ£€æŸ¥ä»·æ ¼æ•°æ®æ˜¯å¦æœ‰æ•ˆ
                if 'Close' in hist.columns:
                    recent_prices = hist['Close'].tail(10)  # æœ€è¿‘10å¤©ä»·æ ¼
                    if recent_prices.isna().all() or (recent_prices <= 0).any():
                        return None, 1  # ä»·æ ¼æ•°æ®æ— æ•ˆï¼Œè·³è¿‡åˆ†æ
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆæ•°æ®ç‚¹
                if len(hist.dropna()) < 20:  # è‡³å°‘éœ€è¦20ä¸ªæœ‰æ•ˆæ•°æ®ç‚¹
                    return None, 1  # æ•°æ®ç‚¹ä¸è¶³ï¼Œè·³è¿‡åˆ†æ
            
            # åŸ·è¡Œæ‰€æœ‰ç­–ç•¥æˆ–è·³éç­–ç•¥
            passed_strategies = []
            if skip_strategies:
                # å¦‚æœè·³éç­–ç•¥ï¼Œå‰‡æ‰€æœ‰è‚¡ç¥¨éƒ½æ¨™è¨˜ç‚ºé€šéç©ºç­–ç•¥åˆ—è¡¨
                passed_strategies = ["è·³éç­–ç•¥"]
                print(f"\r{' ' * 80}\rğŸ” {symbol} å·²è·³éç­–ç•¥ç¯©é¸ï¼Œç›´æ¥é€²è¡ŒAIåˆ†æ")
            else:
                # åŸ·è¡Œæ‰€æœ‰ç­–ç•¥
                for strategy in strategies_to_run:
                    if strategy.run(hist.copy(), info=info, market_return=market_latest_return, is_market_healthy=is_market_healthy):
                        passed_strategies.append(strategy.name)
            
            # æ— è®ºæ˜¯å¦è·³è¿‡ç­–ç•¥ï¼Œåªè¦é€šè¿‡äº†åŸºç¡€ç­›é€‰ï¼Œéƒ½éœ€è¦è¿›è¡ŒAIåˆ†æ
            if passed_strategies or skip_strategies:
            
                # æ­¥éª¤ 1: AI åˆ†æ
                ai_analysis = None
                
                try:
                    ai_analysis = analyze_stock_with_ai({
                        'symbol': symbol,
                        'strategies': passed_strategies,
                        'info': info,
                        'market': market
                    }, hist, interval, model)
                except Exception as ai_e:
                    print(f" - AI åˆ†æå‡ºé”™: {ai_e}", end='')

                # æ­¥éª¤ 2: å°†è‚¡ç¥¨æ·»åŠ åˆ°ç»“æœä¸­ï¼ˆå½“å¯ç”¨ skip_strategies æ—¶ï¼Œæ‰€æœ‰è‚¡ç¥¨éƒ½æ·»åŠ ï¼‰
                # å¦‚æœå¯ç”¨äº† skip_strategiesï¼Œæ‰€æœ‰é€šè¿‡åŸºç¡€ç­›é€‰çš„è‚¡ç¥¨éƒ½æ·»åŠ åˆ°ç»“æœä¸­
                exchange = info.get('exchange', 'UNKNOWN')
                stock_result = {
                    'symbol': symbol,
                    'exchange': exchange,
                    'strategies': passed_strategies,
                    'info': info,
                    'news': news,
                    'ai_analysis': ai_analysis
                }
                
                # å®æ—¶è¾“å‡ºç¬¦åˆæ¡ä»¶çš„è‚¡ç¥¨åˆ°ä¸»æŠ¥å‘Šæ–‡ä»¶
                if realtime_output_enabled:
                    with threading.Lock():
                        with open(main_report_file, 'a', encoding='utf-8') as f:
                            f.write(f"\n--- å¯¦æ™‚è¼¸å‡º ({datetime.now().strftime('%H:%M:%S')}) ---\n")
                            f.write(f"{symbol} ç¬¦åˆç­–ç•¥: {passed_strategies}\n")
                            if ai_analysis:
                                f.write(f"AI åˆ†æ: {ai_analysis['summary']}\n")
                            f.write("-" * 50 + "\n")
                
                if skip_strategies:
                    print(f"\r{' ' * 80}\râœ… {symbol} è·³éç­–ç•¥ç¯©é¸ï¼Œå·²é€²è¡ŒAIåˆ†æ")
                else:
                    print(f"\r{' ' * 80}\râœ… {symbol} ç¬¦åˆç­–ç•¥: {passed_strategies}")
                # ä»…è¾“å‡ºç®€è¦AIåˆ†æä¿¡æ¯ï¼Œè¯¦ç»†å†…å®¹åœ¨æœ€ç»ˆæŠ¥å‘Šä¸­æ˜¾ç¤º
                if ai_analysis:
                    print(f"   ğŸ¤– AI åˆ†æ: å·²å®Œæˆ (æ¨¡å‹: {ai_analysis['model_used']})")
                else:
                    print(f"   ğŸ¤– AI åˆ†æ: æœªèƒ½å®Œæˆ")
                return stock_result, 1
            else:
                return None, 1  # è¿”å›Noneè¡¨ç¤ºè¯¥è‚¡ç¥¨æœªé€šè¿‡ç­–ç•¥ï¼Œä½†å·²åˆ†ææˆåŠŸ
        except Exception as e:
            print(f"\r{' ' * 80}\râŒ åˆ†æ {symbol} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            return None, 0  # è¿”å›0è¡¨ç¤ºåˆ†æå¤±è´¥

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†æ‰€æœ‰è‚¡ç¥¨
    analyzed_count = 0
    qualified_count = 0
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡ï¼Œä¼ é€’é…ç½®å‚æ•°ã€skip_strategieså‚æ•°å’Œmodelå‚æ•°
        future_to_symbol = {executor.submit(analyze_single_stock, symbol, config, skip_strategies, model): symbol for symbol in tickers}
        
        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        for future in as_completed(future_to_symbol):
            symbol = future_to_symbol[future]
            try:
                result, count = future.result()
                if result is not None:
                    qualified_stocks.append(result)
                    qualified_count += 1
                if count > 0:
                    analyzed_count += count
            except Exception as e:
                print(f"\r{' ' * 80}\râŒ è™•ç† {symbol} çš„çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            
            # è®¡ç®—é¢„ä¼°å®Œæˆæ—¶é—´
            elapsed_time = time.time() - start_time
            if analyzed_count > 0:
                avg_time_per_stock = elapsed_time / analyzed_count
                estimated_total_time = avg_time_per_stock * total_stocks
                remaining_time = estimated_total_time - elapsed_time
                remaining_minutes = max(0, int(remaining_time / 60))
            else:
                remaining_minutes = -1  # æœªå¼€å§‹è®¡ç®—
            
            # æ›´æ–°è¿›åº¦
            progress = analyzed_count / total_stocks
            if remaining_minutes >= 0:
                print(f"\råˆ†æé€²åº¦: [{int(progress * 20) * '#'}{int((1 - progress) * 20) * '-'}] {analyzed_count}/{total_stocks} å·²åˆ†æ, {qualified_count} ç¬¦åˆæ¢ä»¶, é ä¼°å‰©é¤˜: {remaining_minutes} åˆ†é˜", end='')
            else:
                print(f"\råˆ†æé€²åº¦: [{int(progress * 20) * '#'}{int((1 - progress) * 20) * '-'}] {analyzed_count}/{total_stocks} å·²åˆ†æ, {qualified_count} ç¬¦åˆæ¢ä»¶", end='')

    # --- æ›´æ–°ç·©å­˜ç‰ˆæœ¬æ–‡ä»¶ ---
    if is_sync_needed:
        print(f"\n--- æ›´æ–°ç·©å­˜ç‰ˆæœ¬è‡³ {today_str} ---")
        with open(version_file, 'w') as f:
            f.write(today_str)
    
    print(f"\n--- åˆ†æå®Œæˆï¼æˆåŠŸåˆ†æ {analyzed_count}/{total_stocks} æ”¯è‚¡ç¥¨ï¼Œæ‰¾åˆ° {len(qualified_stocks)} æ”¯ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨ ---")
    print(f"--- ç¸½è€—æ™‚: {int((time.time() - start_time) / 60)} åˆ†é˜ {int((time.time() - start_time) % 60)} ç§’ ---")
    return qualified_stocks